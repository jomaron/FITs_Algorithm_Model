{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2906069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_size=0.2):\n",
    "    \"\"\"\n",
    "    保证训练集与测试集的类别比例与原数据集中的相等\n",
    "    :param data: 原数据\n",
    "    :param test_size: 测试集比例\n",
    "    :return: 训练集与测试集\n",
    "    \"\"\"\n",
    "    label = set(data.iloc[:, 1])\n",
    "    data_tr = pd.DataFrame()\n",
    "    data_te = pd.DataFrame()\n",
    "    for i in label:\n",
    "        data_i = data[data.iloc[:, 1] == i]\n",
    "        # 标签是i的数据集长度\n",
    "        length = len(data_i)\n",
    "        # 切割的数据长度\n",
    "        split_length = math.floor(length * test_size)\n",
    "        tr = data_i.iloc[:split_length, :]\n",
    "        te = data_i.iloc[split_length:, :]\n",
    "        data_tr = data_tr.append(tr)\n",
    "        data_te = data_te.append(te)\n",
    "\n",
    "    return data_tr,data_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73ef78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "data = pd.read_csv(\"D:/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bd13a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=split_train_test(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "485a8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=a[0]\n",
    "test=a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b62849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature_1646  feature_3311  feature_823  feature_2622  feature_2571  \\\n",
      "0       -0.033986      0.088483    -0.130883     -0.639725     -0.065765   \n",
      "6       -0.049626     -0.093149     0.165083     -0.355241     -0.017614   \n",
      "14      -0.187561      0.040914    -0.071337     -0.342761     -0.001787   \n",
      "36      -0.135453      0.083936    -0.198294     -0.541393      0.083450   \n",
      "37       0.007957     -0.033759    -0.066951     -0.381522      0.047012   \n",
      "..            ...           ...          ...           ...           ...   \n",
      "243     -0.051158     -0.161407     0.048197     -0.690664      0.015340   \n",
      "258     -0.023507      0.077809    -0.037105     -0.751877      0.041784   \n",
      "294     -0.047706     -0.083655     0.020853     -0.734743      0.011660   \n",
      "393     -0.195158     -0.208331     0.196523      0.092955      0.051169   \n",
      "496     -0.095883     -0.188052     0.101960     -0.245077      0.102071   \n",
      "\n",
      "     feature_1790  feature_3254  feature_1044  feature_1618  feature_988  ...  \\\n",
      "0       -0.050212      0.003477      0.068944      0.107171     0.250429  ...   \n",
      "6        0.023888     -0.022398      0.128173      0.103562     0.414540  ...   \n",
      "14       0.068143     -0.000384      0.005782     -0.016418     0.290494  ...   \n",
      "36      -0.001524      0.037303     -0.045097      0.008308     0.411983  ...   \n",
      "37       0.094401      0.145257     -0.039401      0.014436     0.514318  ...   \n",
      "..            ...           ...           ...           ...          ...  ...   \n",
      "243      0.012838     -0.046230      0.106604      0.029839     0.395343  ...   \n",
      "258      0.018431      0.023630     -0.124954      0.104623     0.331599  ...   \n",
      "294     -0.002186      0.035914      0.058330      0.128840     0.439277  ...   \n",
      "393      0.008474     -0.140925      0.027154      0.024958     0.271187  ...   \n",
      "496      0.031890     -0.068505      0.087537      0.129854     0.387368  ...   \n",
      "\n",
      "     feature_2099  feature_3750  feature_3801  feature_2644  feature_1375  \\\n",
      "0       -0.079289      0.023933     -0.024243     -0.002321      0.194415   \n",
      "6        0.013289     -0.073865      0.000655     -0.015085      0.309123   \n",
      "14      -0.004954      0.000731      0.022502     -0.045539      0.035034   \n",
      "36      -0.043078      0.011132     -0.030542     -0.024313      0.021419   \n",
      "37       0.061842      0.038319      0.002223     -0.081210      0.161533   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "243      0.019102     -0.018906     -0.005320     -0.025698      0.390939   \n",
      "258     -0.011119     -0.020572      0.074557      0.031636      0.124282   \n",
      "294     -0.049394     -0.066566      0.069605     -0.017961      0.412137   \n",
      "393     -0.045555     -0.139938      0.051779     -0.030002      0.480812   \n",
      "496     -0.021443     -0.062074      0.037299     -0.026013      0.439184   \n",
      "\n",
      "     feature_2133  feature_666  feature_750  feature_317  feature_1615  \n",
      "0        0.053668     0.175575     0.259276     0.201243     -0.055830  \n",
      "6        0.135017     0.060474    -0.004313     0.458354     -0.088630  \n",
      "14       0.082991     0.140414     0.237071     0.097714     -0.020461  \n",
      "36       0.193469    -0.005389     0.254033     0.163881     -0.041541  \n",
      "37       0.152669     0.207671     0.188693     0.141884      0.071417  \n",
      "..            ...          ...          ...          ...           ...  \n",
      "243     -0.054170     0.135630    -0.161158     0.268711      0.078599  \n",
      "258      0.100060     0.129454     0.142263     0.170856     -0.003689  \n",
      "294      0.091448     0.118186     0.189609     0.336944     -0.077436  \n",
      "393     -0.003656    -0.104728    -0.191806     0.276027     -0.030008  \n",
      "496      0.010906     0.103258     0.130843     0.107106     -0.041434  \n",
      "\n",
      "[495 rows x 61 columns]\n",
      "0      0\n",
      "6      0\n",
      "14     0\n",
      "36     0\n",
      "37     0\n",
      "      ..\n",
      "243    7\n",
      "258    7\n",
      "294    7\n",
      "393    7\n",
      "496    7\n",
      "Name: label, Length: 495, dtype: int64\n",
      "      feature_1646  feature_3311  feature_823  feature_2622  feature_2571  \\\n",
      "451      -0.074414     -0.122372     0.030024     -0.745661     -0.023625   \n",
      "452       0.027990      0.144791    -0.152426     -0.566495      0.070687   \n",
      "457      -0.037258     -0.049479    -0.144444     -0.767413     -0.005806   \n",
      "468      -0.067812     -0.094411    -0.160093     -0.328084      0.076154   \n",
      "491       0.013312     -0.007934    -0.081536     -0.406291     -0.032551   \n",
      "...            ...           ...          ...           ...           ...   \n",
      "2168     -0.096696     -0.097277    -0.021053     -0.717220     -0.011964   \n",
      "2203     -0.073172     -0.284558     0.296407     -0.510710      0.064933   \n",
      "2358     -0.091722     -0.152539     0.091867     -0.718697      0.052858   \n",
      "2425     -0.058882     -0.185114     0.190993     -0.415464      0.069964   \n",
      "2452     -0.094445     -0.079591    -0.043106     -0.716282      0.040754   \n",
      "\n",
      "      feature_1790  feature_3254  feature_1044  feature_1618  feature_988  \\\n",
      "451      -0.000848      0.090169      0.150327      0.143652     0.505975   \n",
      "452       0.006228      0.062750     -0.238580      0.024725     0.593511   \n",
      "457       0.010408      0.056843      0.043941      0.031045     0.299112   \n",
      "468       0.097643      0.005021      0.123678     -0.034109     0.464580   \n",
      "491       0.080101      0.112354     -0.063661      0.008093     0.516031   \n",
      "...            ...           ...           ...           ...          ...   \n",
      "2168      0.008606     -0.003669      0.082710      0.073592     0.290889   \n",
      "2203      0.001377     -0.051852      0.036514      0.079527     0.422419   \n",
      "2358      0.009137     -0.023992      0.072963      0.046716     0.313112   \n",
      "2425     -0.016442     -0.060811      0.134488      0.101014     0.365144   \n",
      "2452     -0.016065      0.001080      0.180835      0.087281     0.357113   \n",
      "\n",
      "      ...  feature_2099  feature_3750  feature_3801  feature_2644  \\\n",
      "451   ...     -0.037678     -0.118325      0.085137     -0.011147   \n",
      "452   ...      0.181899     -0.053562      0.058318     -0.109484   \n",
      "457   ...     -0.108451     -0.004983      0.055065     -0.002622   \n",
      "468   ...      0.082567     -0.024519      0.002356     -0.029727   \n",
      "491   ...      0.062974     -0.065258     -0.003539     -0.061140   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "2168  ...     -0.021845     -0.046793      0.015848     -0.017222   \n",
      "2203  ...     -0.105889     -0.112224      0.071492      0.003967   \n",
      "2358  ...     -0.063290     -0.048939      0.047297     -0.003836   \n",
      "2425  ...     -0.069233     -0.087516      0.020792     -0.001769   \n",
      "2452  ...     -0.019944     -0.082052      0.054682     -0.020704   \n",
      "\n",
      "      feature_1375  feature_2133  feature_666  feature_750  feature_317  \\\n",
      "451       0.454654      0.073628    -0.117471     0.129065     0.415559   \n",
      "452       0.002192      0.285664     0.021160     0.046963     0.417889   \n",
      "457       0.278683      0.209002     0.106507     0.253433     0.004372   \n",
      "468       0.174812      0.152613    -0.144739     0.167192     0.361866   \n",
      "491       0.214319      0.257404     0.182998     0.175722     0.164313   \n",
      "...            ...           ...          ...          ...          ...   \n",
      "2168      0.326313      0.061388     0.216970    -0.017159     0.118191   \n",
      "2203      0.462421     -0.005195     0.057391    -0.123220     0.471932   \n",
      "2358      0.430447      0.028028     0.004861    -0.138240     0.201156   \n",
      "2425      0.446435      0.112645    -0.053681    -0.156944     0.495205   \n",
      "2452      0.336194      0.030333     0.134094     0.073410     0.100189   \n",
      "\n",
      "      feature_1615  \n",
      "451      -0.064765  \n",
      "452       0.049060  \n",
      "457      -0.041034  \n",
      "468      -0.030776  \n",
      "491       0.066175  \n",
      "...            ...  \n",
      "2168      0.047579  \n",
      "2203     -0.012200  \n",
      "2358     -0.002593  \n",
      "2425     -0.101996  \n",
      "2452      0.008879  \n",
      "\n",
      "[1994 rows x 61 columns]\n",
      "451     0\n",
      "452     0\n",
      "457     0\n",
      "468     0\n",
      "491     0\n",
      "       ..\n",
      "2168    7\n",
      "2203    7\n",
      "2358    7\n",
      "2425    7\n",
      "2452    7\n",
      "Name: label, Length: 1994, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_X = train.iloc[:,2:]\n",
    "train_Y = train.iloc[:,1]\n",
    "\n",
    "test_X = test.iloc[:,2:]\n",
    "test_Y = test.iloc[:,1]\n",
    "print(train_X)\n",
    "print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac196f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xg_train = xgb.DMatrix(train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 1\n",
    "param['max_depth'] = 6\n",
    "#param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd3bb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.74092\ttest-mlogloss:1.58982\n",
      "[1]\ttrain-mlogloss:0.32908\ttest-mlogloss:1.60248\n",
      "[2]\ttrain-mlogloss:0.17082\ttest-mlogloss:1.61419\n",
      "[3]\ttrain-mlogloss:0.10744\ttest-mlogloss:1.64253\n",
      "[4]\ttrain-mlogloss:0.07384\ttest-mlogloss:1.65865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttrain-mlogloss:0.05463\ttest-mlogloss:1.67817\n",
      "[6]\ttrain-mlogloss:0.04335\ttest-mlogloss:1.68434\n",
      "[7]\ttrain-mlogloss:0.03596\ttest-mlogloss:1.70196\n",
      "[8]\ttrain-mlogloss:0.03101\ttest-mlogloss:1.71203\n",
      "[9]\ttrain-mlogloss:0.02719\ttest-mlogloss:1.73065\n",
      "[10]\ttrain-mlogloss:0.02444\ttest-mlogloss:1.74257\n",
      "[11]\ttrain-mlogloss:0.02238\ttest-mlogloss:1.75336\n",
      "[12]\ttrain-mlogloss:0.02081\ttest-mlogloss:1.76324\n",
      "[13]\ttrain-mlogloss:0.01959\ttest-mlogloss:1.76874\n",
      "[14]\ttrain-mlogloss:0.01841\ttest-mlogloss:1.77295\n",
      "[15]\ttrain-mlogloss:0.01749\ttest-mlogloss:1.78136\n",
      "[16]\ttrain-mlogloss:0.01672\ttest-mlogloss:1.79056\n",
      "[17]\ttrain-mlogloss:0.01602\ttest-mlogloss:1.79234\n",
      "[18]\ttrain-mlogloss:0.01547\ttest-mlogloss:1.79716\n",
      "[19]\ttrain-mlogloss:0.01493\ttest-mlogloss:1.80084\n",
      "[20]\ttrain-mlogloss:0.01448\ttest-mlogloss:1.80401\n",
      "[21]\ttrain-mlogloss:0.01411\ttest-mlogloss:1.80965\n",
      "[22]\ttrain-mlogloss:0.01375\ttest-mlogloss:1.81281\n",
      "[23]\ttrain-mlogloss:0.01346\ttest-mlogloss:1.81937\n",
      "[24]\ttrain-mlogloss:0.01315\ttest-mlogloss:1.81946\n",
      "[25]\ttrain-mlogloss:0.01285\ttest-mlogloss:1.82247\n",
      "[26]\ttrain-mlogloss:0.01261\ttest-mlogloss:1.82681\n",
      "[27]\ttrain-mlogloss:0.01239\ttest-mlogloss:1.82991\n",
      "[28]\ttrain-mlogloss:0.01217\ttest-mlogloss:1.83132\n",
      "[29]\ttrain-mlogloss:0.01198\ttest-mlogloss:1.83058\n",
      "[30]\ttrain-mlogloss:0.01175\ttest-mlogloss:1.83365\n",
      "[31]\ttrain-mlogloss:0.01157\ttest-mlogloss:1.83537\n",
      "[32]\ttrain-mlogloss:0.01139\ttest-mlogloss:1.83837\n",
      "[33]\ttrain-mlogloss:0.01124\ttest-mlogloss:1.83923\n",
      "[34]\ttrain-mlogloss:0.01110\ttest-mlogloss:1.84263\n",
      "[35]\ttrain-mlogloss:0.01096\ttest-mlogloss:1.84470\n",
      "[36]\ttrain-mlogloss:0.01081\ttest-mlogloss:1.84538\n",
      "[37]\ttrain-mlogloss:0.01070\ttest-mlogloss:1.84796\n",
      "[38]\ttrain-mlogloss:0.01059\ttest-mlogloss:1.85317\n",
      "[39]\ttrain-mlogloss:0.01049\ttest-mlogloss:1.85328\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 40\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist);\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c937b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.43229689067201604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "error_rate = np.sum(pred != test_Y) / len(test_Y)\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25fa0318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.74092\ttest-mlogloss:1.58982\n",
      "[1]\ttrain-mlogloss:0.32908\ttest-mlogloss:1.60248\n",
      "[2]\ttrain-mlogloss:0.17082\ttest-mlogloss:1.61419\n",
      "[3]\ttrain-mlogloss:0.10744\ttest-mlogloss:1.64253\n",
      "[4]\ttrain-mlogloss:0.07384\ttest-mlogloss:1.65865\n",
      "[5]\ttrain-mlogloss:0.05463\ttest-mlogloss:1.67817\n",
      "[6]\ttrain-mlogloss:0.04335\ttest-mlogloss:1.68434\n",
      "[7]\ttrain-mlogloss:0.03596\ttest-mlogloss:1.70196\n",
      "[8]\ttrain-mlogloss:0.03101\ttest-mlogloss:1.71203\n",
      "[9]\ttrain-mlogloss:0.02719\ttest-mlogloss:1.73065\n",
      "[10]\ttrain-mlogloss:0.02444\ttest-mlogloss:1.74257\n",
      "[11]\ttrain-mlogloss:0.02238\ttest-mlogloss:1.75336\n",
      "[12]\ttrain-mlogloss:0.02081\ttest-mlogloss:1.76324\n",
      "[13]\ttrain-mlogloss:0.01959\ttest-mlogloss:1.76874\n",
      "[14]\ttrain-mlogloss:0.01841\ttest-mlogloss:1.77295\n",
      "[15]\ttrain-mlogloss:0.01749\ttest-mlogloss:1.78136\n",
      "[16]\ttrain-mlogloss:0.01672\ttest-mlogloss:1.79056\n",
      "[17]\ttrain-mlogloss:0.01602\ttest-mlogloss:1.79234\n",
      "[18]\ttrain-mlogloss:0.01547\ttest-mlogloss:1.79716\n",
      "[19]\ttrain-mlogloss:0.01493\ttest-mlogloss:1.80084\n",
      "[20]\ttrain-mlogloss:0.01448\ttest-mlogloss:1.80401\n",
      "[21]\ttrain-mlogloss:0.01411\ttest-mlogloss:1.80965\n",
      "[22]\ttrain-mlogloss:0.01375\ttest-mlogloss:1.81281\n",
      "[23]\ttrain-mlogloss:0.01346\ttest-mlogloss:1.81937\n",
      "[24]\ttrain-mlogloss:0.01315\ttest-mlogloss:1.81946\n",
      "[25]\ttrain-mlogloss:0.01285\ttest-mlogloss:1.82247\n",
      "[26]\ttrain-mlogloss:0.01261\ttest-mlogloss:1.82681\n",
      "[27]\ttrain-mlogloss:0.01239\ttest-mlogloss:1.82991\n",
      "[28]\ttrain-mlogloss:0.01217\ttest-mlogloss:1.83132\n",
      "[29]\ttrain-mlogloss:0.01198\ttest-mlogloss:1.83058\n",
      "[30]\ttrain-mlogloss:0.01175\ttest-mlogloss:1.83365\n",
      "[31]\ttrain-mlogloss:0.01157\ttest-mlogloss:1.83537\n",
      "[32]\ttrain-mlogloss:0.01139\ttest-mlogloss:1.83837\n",
      "[33]\ttrain-mlogloss:0.01124\ttest-mlogloss:1.83923\n",
      "[34]\ttrain-mlogloss:0.01110\ttest-mlogloss:1.84263\n",
      "[35]\ttrain-mlogloss:0.01096\ttest-mlogloss:1.84470\n",
      "[36]\ttrain-mlogloss:0.01081\ttest-mlogloss:1.84538\n",
      "[37]\ttrain-mlogloss:0.01070\ttest-mlogloss:1.84796\n",
      "[38]\ttrain-mlogloss:0.01059\ttest-mlogloss:1.85317\n",
      "[39]\ttrain-mlogloss:0.01049\ttest-mlogloss:1.85328\n",
      "Test error using softprob = 0.43229689067201604\n"
     ]
    }
   ],
   "source": [
    "param['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bst.predict(xg_test).reshape(len(test_Y), 8)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != test_Y) / len(test_Y)\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03a1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58feba8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
