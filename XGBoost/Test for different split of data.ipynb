{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2906069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label  feature_1646  feature_3311  feature_823  feature_2622  \\\n",
      "1555      4     -0.077750     -0.275683     0.108040     -0.605429   \n",
      "2105      3      0.026605      0.324206    -0.021528     -0.577969   \n",
      "140       0     -0.027260     -0.025359     0.027317     -0.425197   \n",
      "106       3     -0.053214      0.108838    -0.146424     -0.092960   \n",
      "767       1      0.008146     -0.035302    -0.308727     -0.751416   \n",
      "...     ...           ...           ...          ...           ...   \n",
      "436       1     -0.025050     -0.031085     0.020445     -0.359408   \n",
      "1083      1     -0.034606     -0.102083     0.012192     -0.718343   \n",
      "1340      1      0.065795     -0.044693     0.057861     -0.701758   \n",
      "1138      4     -0.255232      0.138174     0.164621     -0.610590   \n",
      "214       4     -0.157969     -0.137029    -0.076178     -0.384696   \n",
      "\n",
      "      feature_2571  feature_1790  feature_3254  feature_1044  feature_1618  \\\n",
      "1555      0.048647     -0.008054     -0.061222      0.102992      0.070770   \n",
      "2105      0.004075      0.037846      0.086754     -0.172952      0.090108   \n",
      "140       0.018352     -0.029280      0.004402     -0.082846      0.140468   \n",
      "106       0.119638      0.043210     -0.028550     -0.229404      0.068608   \n",
      "767       0.071756      0.028964      0.064390      0.086297      0.032986   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "436       0.024597      0.122349      0.117723      0.038198      0.047689   \n",
      "1083      0.099487      0.046075     -0.002636      0.133319      0.050669   \n",
      "1340     -0.029624      0.043896      0.072333      0.102979      0.124625   \n",
      "1138      0.082988      0.214867     -0.079173     -0.281347      0.111060   \n",
      "214       0.142198      0.121719     -0.036751      0.213569     -0.064969   \n",
      "\n",
      "      ...  feature_2099  feature_3750  feature_3801  feature_2644  \\\n",
      "1555  ...     -0.107785     -0.111019      0.051007      0.017426   \n",
      "2105  ...      0.129447      0.084000     -0.018979     -0.058607   \n",
      "140   ...     -0.113013     -0.022713      0.062569     -0.020151   \n",
      "106   ...      0.018233     -0.026339      0.003773     -0.014073   \n",
      "767   ...     -0.051465     -0.043626     -0.003734      0.001663   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "436   ...      0.124482     -0.012371     -0.038519     -0.074807   \n",
      "1083  ...     -0.035453     -0.059453      0.049974     -0.029173   \n",
      "1340  ...     -0.008058     -0.057746      0.047717     -0.014925   \n",
      "1138  ...     -0.022952      0.019069     -0.085606     -0.115485   \n",
      "214   ...     -0.029947     -0.165019     -0.031522     -0.114639   \n",
      "\n",
      "      feature_1375  feature_2133  feature_666  feature_750  feature_317  \\\n",
      "1555      0.398365     -0.070459     0.009948    -0.146358     0.395844   \n",
      "2105      0.054842      0.212773     0.121862    -0.006935     0.282513   \n",
      "140       0.146531      0.264800     0.176825     0.070154     0.267402   \n",
      "106      -0.129373      0.263553     0.170458     0.213154    -0.002941   \n",
      "767       0.217332      0.050232     0.139360     0.211344     0.082024   \n",
      "...            ...           ...          ...          ...          ...   \n",
      "436       0.066779      0.182009     0.147266     0.176059     0.206486   \n",
      "1083      0.343226      0.098068     0.195137     0.055014     0.262471   \n",
      "1340      0.479820      0.012088     0.170953     0.108778     0.256879   \n",
      "1138     -0.177103     -0.086597     0.315451     0.099070     0.044156   \n",
      "214       0.174024      0.119317    -0.267601     0.016223     0.383306   \n",
      "\n",
      "      feature_1615  \n",
      "1555      0.045912  \n",
      "2105      0.025900  \n",
      "140       0.057215  \n",
      "106       0.002655  \n",
      "767      -0.003973  \n",
      "...            ...  \n",
      "436       0.000324  \n",
      "1083     -0.060760  \n",
      "1340     -0.016304  \n",
      "1138      0.194201  \n",
      "214       0.067075  \n",
      "\n",
      "[1991 rows x 62 columns]\n",
      "      label  feature_1646  feature_3311  feature_823  feature_2622  \\\n",
      "618       1     -0.078138     -0.191988     0.036232     -0.731078   \n",
      "2191      1      0.110973      0.015328     0.051695     -0.387678   \n",
      "1229      1      0.124053     -0.011949    -0.225647     -0.289526   \n",
      "676       2     -0.151981     -0.135154    -0.140752     -0.449499   \n",
      "1824      5     -0.096893     -0.204214    -0.039526     -0.694334   \n",
      "...     ...           ...           ...          ...           ...   \n",
      "600       1      0.102041      0.031528    -0.303098     -0.818155   \n",
      "1111      4     -0.084375     -0.037590    -0.278431     -0.395733   \n",
      "2233      3     -0.088035      0.002493    -0.198059     -0.599788   \n",
      "1005      1     -0.178460      0.025111    -0.209395     -0.545886   \n",
      "1885      3      0.137097     -0.011902    -0.223778     -0.761285   \n",
      "\n",
      "      feature_2571  feature_1790  feature_3254  feature_1044  feature_1618  \\\n",
      "618       0.029893     -0.023932     -0.021684      0.128835      0.075537   \n",
      "2191      0.043389     -0.009283      0.012930      0.115622      0.080398   \n",
      "1229     -0.014246     -0.011487     -0.143443     -0.024272      0.067305   \n",
      "676       0.088593      0.033117      0.010551      0.148558      0.022107   \n",
      "1824      0.106079     -0.003564     -0.025080      0.103713     -0.000494   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "600      -0.006499     -0.052609      0.033761      0.091805      0.150456   \n",
      "1111      0.046268     -0.015278      0.008725      0.073407     -0.069140   \n",
      "2233      0.037382     -0.042009      0.021797      0.041428     -0.034263   \n",
      "1005      0.052728      0.016241      0.046320     -0.026519     -0.016164   \n",
      "1885      0.062877     -0.091785      0.020977      0.090282      0.133215   \n",
      "\n",
      "      ...  feature_2099  feature_3750  feature_3801  feature_2644  \\\n",
      "618   ...     -0.019632     -0.075297      0.087935      0.055856   \n",
      "2191  ...      0.089983      0.025889     -0.004790     -0.079425   \n",
      "1229  ...      0.023989     -0.001413     -0.013094     -0.016520   \n",
      "676   ...     -0.043749     -0.079847     -0.065312     -0.028962   \n",
      "1824  ...     -0.093300     -0.063526      0.070012      0.009281   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "600   ...      0.004219      0.008618      0.049262     -0.013151   \n",
      "1111  ...     -0.028958     -0.015724      0.015265     -0.084389   \n",
      "2233  ...     -0.025309     -0.036149     -0.017611     -0.029445   \n",
      "1005  ...     -0.077801     -0.015798      0.063065     -0.054671   \n",
      "1885  ...      0.090266     -0.049247      0.013581     -0.030012   \n",
      "\n",
      "      feature_1375  feature_2133  feature_666  feature_750  feature_317  \\\n",
      "618       0.376672     -0.028744     0.065844    -0.148591     0.344980   \n",
      "2191     -0.041288      0.175069     0.175692     0.196379     0.408289   \n",
      "1229      0.104051      0.178185     0.007952     0.074398     0.288851   \n",
      "676       0.056863      0.233409    -0.028428     0.199940     0.317559   \n",
      "1824      0.346696     -0.015294     0.121133    -0.122458     0.396860   \n",
      "...            ...           ...          ...          ...          ...   \n",
      "600       0.164860      0.043398     0.200167     0.092514     0.306900   \n",
      "1111     -0.140249      0.144653     0.113878     0.101147     0.282655   \n",
      "2233      0.330773      0.166518    -0.074383     0.109007     0.089771   \n",
      "1005     -0.123721      0.243260     0.152687     0.190116     0.079851   \n",
      "1885      0.306368      0.020412     0.244314     0.020777     0.348805   \n",
      "\n",
      "      feature_1615  \n",
      "618       0.008302  \n",
      "2191      0.005886  \n",
      "1229      0.095596  \n",
      "676      -0.032275  \n",
      "1824      0.006991  \n",
      "...            ...  \n",
      "600       0.111866  \n",
      "1111      0.050162  \n",
      "2233     -0.088310  \n",
      "1005      0.020417  \n",
      "1885      0.187470  \n",
      "\n",
      "[498 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "df = pd.read_csv('D:/train.csv', encoding= 'utf-8')\n",
    "\n",
    "X = df.iloc[:,2:]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=70)\n",
    "\n",
    "df1 = pd.concat([y_train, X_train], axis=1)\n",
    "df2 = pd.concat([y_test, X_test], axis=1)\n",
    "print(df1)\n",
    "print(df2)\n",
    "#df1.to_csv('D:/train_final.csv')\n",
    "#df2.to_csv('D:/devel_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "485a8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df1\n",
    "test=df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b62849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_1646  feature_3311  feature_823  feature_2622  feature_2571  \\\n",
      "1555     -0.077750     -0.275683     0.108040     -0.605429      0.048647   \n",
      "2105      0.026605      0.324206    -0.021528     -0.577969      0.004075   \n",
      "140      -0.027260     -0.025359     0.027317     -0.425197      0.018352   \n",
      "106      -0.053214      0.108838    -0.146424     -0.092960      0.119638   \n",
      "767       0.008146     -0.035302    -0.308727     -0.751416      0.071756   \n",
      "...            ...           ...          ...           ...           ...   \n",
      "436      -0.025050     -0.031085     0.020445     -0.359408      0.024597   \n",
      "1083     -0.034606     -0.102083     0.012192     -0.718343      0.099487   \n",
      "1340      0.065795     -0.044693     0.057861     -0.701758     -0.029624   \n",
      "1138     -0.255232      0.138174     0.164621     -0.610590      0.082988   \n",
      "214      -0.157969     -0.137029    -0.076178     -0.384696      0.142198   \n",
      "\n",
      "      feature_1790  feature_3254  feature_1044  feature_1618  feature_988  \\\n",
      "1555     -0.008054     -0.061222      0.102992      0.070770     0.346078   \n",
      "2105      0.037846      0.086754     -0.172952      0.090108     0.423695   \n",
      "140      -0.029280      0.004402     -0.082846      0.140468     0.477528   \n",
      "106       0.043210     -0.028550     -0.229404      0.068608     0.317541   \n",
      "767       0.028964      0.064390      0.086297      0.032986     0.303988   \n",
      "...            ...           ...           ...           ...          ...   \n",
      "436       0.122349      0.117723      0.038198      0.047689     0.538743   \n",
      "1083      0.046075     -0.002636      0.133319      0.050669     0.399028   \n",
      "1340      0.043896      0.072333      0.102979      0.124625     0.412579   \n",
      "1138      0.214867     -0.079173     -0.281347      0.111060     0.423584   \n",
      "214       0.121719     -0.036751      0.213569     -0.064969     0.487444   \n",
      "\n",
      "      ...  feature_2099  feature_3750  feature_3801  feature_2644  \\\n",
      "1555  ...     -0.107785     -0.111019      0.051007      0.017426   \n",
      "2105  ...      0.129447      0.084000     -0.018979     -0.058607   \n",
      "140   ...     -0.113013     -0.022713      0.062569     -0.020151   \n",
      "106   ...      0.018233     -0.026339      0.003773     -0.014073   \n",
      "767   ...     -0.051465     -0.043626     -0.003734      0.001663   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "436   ...      0.124482     -0.012371     -0.038519     -0.074807   \n",
      "1083  ...     -0.035453     -0.059453      0.049974     -0.029173   \n",
      "1340  ...     -0.008058     -0.057746      0.047717     -0.014925   \n",
      "1138  ...     -0.022952      0.019069     -0.085606     -0.115485   \n",
      "214   ...     -0.029947     -0.165019     -0.031522     -0.114639   \n",
      "\n",
      "      feature_1375  feature_2133  feature_666  feature_750  feature_317  \\\n",
      "1555      0.398365     -0.070459     0.009948    -0.146358     0.395844   \n",
      "2105      0.054842      0.212773     0.121862    -0.006935     0.282513   \n",
      "140       0.146531      0.264800     0.176825     0.070154     0.267402   \n",
      "106      -0.129373      0.263553     0.170458     0.213154    -0.002941   \n",
      "767       0.217332      0.050232     0.139360     0.211344     0.082024   \n",
      "...            ...           ...          ...          ...          ...   \n",
      "436       0.066779      0.182009     0.147266     0.176059     0.206486   \n",
      "1083      0.343226      0.098068     0.195137     0.055014     0.262471   \n",
      "1340      0.479820      0.012088     0.170953     0.108778     0.256879   \n",
      "1138     -0.177103     -0.086597     0.315451     0.099070     0.044156   \n",
      "214       0.174024      0.119317    -0.267601     0.016223     0.383306   \n",
      "\n",
      "      feature_1615  \n",
      "1555      0.045912  \n",
      "2105      0.025900  \n",
      "140       0.057215  \n",
      "106       0.002655  \n",
      "767      -0.003973  \n",
      "...            ...  \n",
      "436       0.000324  \n",
      "1083     -0.060760  \n",
      "1340     -0.016304  \n",
      "1138      0.194201  \n",
      "214       0.067075  \n",
      "\n",
      "[1991 rows x 61 columns]\n",
      "1555    4\n",
      "2105    3\n",
      "140     0\n",
      "106     3\n",
      "767     1\n",
      "       ..\n",
      "436     1\n",
      "1083    1\n",
      "1340    1\n",
      "1138    4\n",
      "214     4\n",
      "Name: label, Length: 1991, dtype: int64\n",
      "      feature_1646  feature_3311  feature_823  feature_2622  feature_2571  \\\n",
      "618      -0.078138     -0.191988     0.036232     -0.731078      0.029893   \n",
      "2191      0.110973      0.015328     0.051695     -0.387678      0.043389   \n",
      "1229      0.124053     -0.011949    -0.225647     -0.289526     -0.014246   \n",
      "676      -0.151981     -0.135154    -0.140752     -0.449499      0.088593   \n",
      "1824     -0.096893     -0.204214    -0.039526     -0.694334      0.106079   \n",
      "...            ...           ...          ...           ...           ...   \n",
      "600       0.102041      0.031528    -0.303098     -0.818155     -0.006499   \n",
      "1111     -0.084375     -0.037590    -0.278431     -0.395733      0.046268   \n",
      "2233     -0.088035      0.002493    -0.198059     -0.599788      0.037382   \n",
      "1005     -0.178460      0.025111    -0.209395     -0.545886      0.052728   \n",
      "1885      0.137097     -0.011902    -0.223778     -0.761285      0.062877   \n",
      "\n",
      "      feature_1790  feature_3254  feature_1044  feature_1618  feature_988  \\\n",
      "618      -0.023932     -0.021684      0.128835      0.075537     0.306118   \n",
      "2191     -0.009283      0.012930      0.115622      0.080398     0.495083   \n",
      "1229     -0.011487     -0.143443     -0.024272      0.067305     0.343780   \n",
      "676       0.033117      0.010551      0.148558      0.022107     0.482321   \n",
      "1824     -0.003564     -0.025080      0.103713     -0.000494     0.405579   \n",
      "...            ...           ...           ...           ...          ...   \n",
      "600      -0.052609      0.033761      0.091805      0.150456     0.467474   \n",
      "1111     -0.015278      0.008725      0.073407     -0.069140     0.397800   \n",
      "2233     -0.042009      0.021797      0.041428     -0.034263     0.463919   \n",
      "1005      0.016241      0.046320     -0.026519     -0.016164     0.282853   \n",
      "1885     -0.091785      0.020977      0.090282      0.133215     0.423440   \n",
      "\n",
      "      ...  feature_2099  feature_3750  feature_3801  feature_2644  \\\n",
      "618   ...     -0.019632     -0.075297      0.087935      0.055856   \n",
      "2191  ...      0.089983      0.025889     -0.004790     -0.079425   \n",
      "1229  ...      0.023989     -0.001413     -0.013094     -0.016520   \n",
      "676   ...     -0.043749     -0.079847     -0.065312     -0.028962   \n",
      "1824  ...     -0.093300     -0.063526      0.070012      0.009281   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "600   ...      0.004219      0.008618      0.049262     -0.013151   \n",
      "1111  ...     -0.028958     -0.015724      0.015265     -0.084389   \n",
      "2233  ...     -0.025309     -0.036149     -0.017611     -0.029445   \n",
      "1005  ...     -0.077801     -0.015798      0.063065     -0.054671   \n",
      "1885  ...      0.090266     -0.049247      0.013581     -0.030012   \n",
      "\n",
      "      feature_1375  feature_2133  feature_666  feature_750  feature_317  \\\n",
      "618       0.376672     -0.028744     0.065844    -0.148591     0.344980   \n",
      "2191     -0.041288      0.175069     0.175692     0.196379     0.408289   \n",
      "1229      0.104051      0.178185     0.007952     0.074398     0.288851   \n",
      "676       0.056863      0.233409    -0.028428     0.199940     0.317559   \n",
      "1824      0.346696     -0.015294     0.121133    -0.122458     0.396860   \n",
      "...            ...           ...          ...          ...          ...   \n",
      "600       0.164860      0.043398     0.200167     0.092514     0.306900   \n",
      "1111     -0.140249      0.144653     0.113878     0.101147     0.282655   \n",
      "2233      0.330773      0.166518    -0.074383     0.109007     0.089771   \n",
      "1005     -0.123721      0.243260     0.152687     0.190116     0.079851   \n",
      "1885      0.306368      0.020412     0.244314     0.020777     0.348805   \n",
      "\n",
      "      feature_1615  \n",
      "618       0.008302  \n",
      "2191      0.005886  \n",
      "1229      0.095596  \n",
      "676      -0.032275  \n",
      "1824      0.006991  \n",
      "...            ...  \n",
      "600       0.111866  \n",
      "1111      0.050162  \n",
      "2233     -0.088310  \n",
      "1005      0.020417  \n",
      "1885      0.187470  \n",
      "\n",
      "[498 rows x 61 columns]\n",
      "618     1\n",
      "2191    1\n",
      "1229    1\n",
      "676     2\n",
      "1824    5\n",
      "       ..\n",
      "600     1\n",
      "1111    4\n",
      "2233    3\n",
      "1005    1\n",
      "1885    3\n",
      "Name: label, Length: 498, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_X = train.iloc[:,1:]\n",
    "train_Y = train.iloc[:,0]\n",
    "\n",
    "test_X = test.iloc[:,1:]\n",
    "test_Y = test.iloc[:,0]\n",
    "print(train_X)\n",
    "print(train_Y)\n",
    "print(test_X)\n",
    "print(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac196f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xg_train = xgb.DMatrix(train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 1\n",
    "param['max_depth'] = 6\n",
    "#param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd3bb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.90447\ttest-mlogloss:1.50108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-mlogloss:0.59279\ttest-mlogloss:1.43615\n",
      "[2]\ttrain-mlogloss:0.38434\ttest-mlogloss:1.43987\n",
      "[3]\ttrain-mlogloss:0.25504\ttest-mlogloss:1.47199\n",
      "[4]\ttrain-mlogloss:0.18108\ttest-mlogloss:1.47226\n",
      "[5]\ttrain-mlogloss:0.13260\ttest-mlogloss:1.45206\n",
      "[6]\ttrain-mlogloss:0.10082\ttest-mlogloss:1.45418\n",
      "[7]\ttrain-mlogloss:0.07537\ttest-mlogloss:1.47660\n",
      "[8]\ttrain-mlogloss:0.05844\ttest-mlogloss:1.48850\n",
      "[9]\ttrain-mlogloss:0.04886\ttest-mlogloss:1.51417\n",
      "[10]\ttrain-mlogloss:0.04116\ttest-mlogloss:1.52514\n",
      "[11]\ttrain-mlogloss:0.03521\ttest-mlogloss:1.52783\n",
      "[12]\ttrain-mlogloss:0.02976\ttest-mlogloss:1.52415\n",
      "[13]\ttrain-mlogloss:0.02611\ttest-mlogloss:1.52661\n",
      "[14]\ttrain-mlogloss:0.02306\ttest-mlogloss:1.53481\n",
      "[15]\ttrain-mlogloss:0.02089\ttest-mlogloss:1.54638\n",
      "[16]\ttrain-mlogloss:0.01926\ttest-mlogloss:1.55703\n",
      "[17]\ttrain-mlogloss:0.01731\ttest-mlogloss:1.56295\n",
      "[18]\ttrain-mlogloss:0.01600\ttest-mlogloss:1.56856\n",
      "[19]\ttrain-mlogloss:0.01474\ttest-mlogloss:1.57009\n",
      "[20]\ttrain-mlogloss:0.01381\ttest-mlogloss:1.58073\n",
      "[21]\ttrain-mlogloss:0.01295\ttest-mlogloss:1.58990\n",
      "[22]\ttrain-mlogloss:0.01217\ttest-mlogloss:1.59751\n",
      "[23]\ttrain-mlogloss:0.01153\ttest-mlogloss:1.60216\n",
      "[24]\ttrain-mlogloss:0.01092\ttest-mlogloss:1.60464\n",
      "[25]\ttrain-mlogloss:0.01039\ttest-mlogloss:1.61268\n",
      "[26]\ttrain-mlogloss:0.00991\ttest-mlogloss:1.61837\n",
      "[27]\ttrain-mlogloss:0.00955\ttest-mlogloss:1.62767\n",
      "[28]\ttrain-mlogloss:0.00910\ttest-mlogloss:1.63171\n",
      "[29]\ttrain-mlogloss:0.00875\ttest-mlogloss:1.63651\n",
      "[30]\ttrain-mlogloss:0.00847\ttest-mlogloss:1.63747\n",
      "[31]\ttrain-mlogloss:0.00818\ttest-mlogloss:1.63946\n",
      "[32]\ttrain-mlogloss:0.00791\ttest-mlogloss:1.64262\n",
      "[33]\ttrain-mlogloss:0.00768\ttest-mlogloss:1.64201\n",
      "[34]\ttrain-mlogloss:0.00746\ttest-mlogloss:1.64881\n",
      "[35]\ttrain-mlogloss:0.00726\ttest-mlogloss:1.64864\n",
      "[36]\ttrain-mlogloss:0.00707\ttest-mlogloss:1.65330\n",
      "[37]\ttrain-mlogloss:0.00688\ttest-mlogloss:1.65075\n",
      "[38]\ttrain-mlogloss:0.00671\ttest-mlogloss:1.65429\n",
      "[39]\ttrain-mlogloss:0.00653\ttest-mlogloss:1.65527\n"
     ]
    }
   ],
   "source": [
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 40\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist);\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d3c47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error using softmax = 0.3674698795180723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "error_rate = np.sum(pred != test_Y) / len(test_Y)\n",
    "accuracy_rate=1-error_rate\n",
    "print('Test accuracy using softmax = {}'.format(accuracy_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8127c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 2.0, 2.0, 1.0, 3.0, 4.0, 1.0, 3.0, 1.0, 1.0, 1.0, 4.0, 1.0, 5.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 5.0, 1.0, 1.0, 2.0, 1.0, 2.0, 3.0, 1.0, 4.0, 4.0, 3.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 2.0, 1.0, 7.0, 1.0, 3.0, 1.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 2.0, 4.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 4.0, 1.0, 0.0, 1.0, 2.0, 3.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 4.0, 4.0, 2.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 3.0, 4.0, 2.0, 1.0, 1.0, 4.0, 3.0, 1.0, 4.0, 3.0, 4.0, 1.0, 4.0, 4.0, 4.0, 5.0, 4.0, 4.0, 3.0, 1.0, 4.0, 1.0, 4.0, 3.0, 2.0, 4.0, 1.0, 2.0, 1.0, 0.0, 4.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 5.0, 4.0, 3.0, 4.0, 4.0, 4.0, 1.0, 4.0, 2.0, 4.0, 1.0, 4.0, 1.0, 1.0, 1.0, 5.0, 4.0, 3.0, 1.0, 1.0, 1.0, 4.0, 4.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 4.0, 1.0, 0.0, 4.0, 1.0, 1.0, 3.0, 2.0, 1.0, 4.0, 1.0, 4.0, 4.0, 3.0, 5.0, 2.0, 0.0, 1.0, 1.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 0.0, 0.0, 1.0, 1.0, 4.0, 4.0, 1.0, 1.0, 4.0, 4.0, 1.0, 4.0, 5.0, 1.0, 1.0, 4.0, 1.0, 2.0, 1.0, 4.0, 4.0, 1.0, 5.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 4.0, 4.0, 4.0, 4.0, 7.0, 4.0, 4.0, 5.0, 1.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 4.0, 1.0, 1.0, 1.0, 4.0, 4.0, 4.0, 1.0, 1.0, 2.0, 1.0, 4.0, 0.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 4.0, 5.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 4.0, 4.0, 3.0, 4.0, 1.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 1.0, 3.0, 2.0, 4.0, 1.0, 4.0, 4.0, 0.0, 1.0, 1.0, 1.0, 7.0, 4.0, 4.0, 3.0, 4.0, 3.0, 1.0, 1.0, 5.0, 3.0, 1.0, 1.0, 4.0, 1.0, 4.0, 5.0, 0.0, 1.0, 4.0, 3.0, 4.0, 1.0, 3.0, 1.0, 5.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 0.0, 1.0, 4.0, 1.0, 1.0, 4.0, 4.0, 1.0, 1.0, 1.0, 4.0, 0.0, 4.0, 4.0, 4.0, 1.0, 4.0, 1.0, 4.0, 1.0, 2.0, 4.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 4.0, 4.0, 4.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 4.0, 1.0, 1.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0, 4.0, 0.0, 1.0, 1.0, 5.0, 4.0, 1.0, 1.0, 4.0, 4.0, 4.0, 4.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 3.0, 1.0, 4.0, 1.0, 4.0, 2.0, 1.0, 4.0, 1.0, 1.0, 1.0, 3.0, 4.0, 3.0, 3.0, 1.0, 4.0, 1.0, 5.0, 4.0, 1.0, 4.0, 1.0, 4.0, 1.0, 4.0, 4.0, 2.0, 4.0, 1.0, 1.0, 4.0, 3.0, 1.0, 1.0, 4.0, 1.0, 3.0, 4.0, 1.0, 1.0, 3.0, 1.0, 4.0, 4.0, 1.0, 4.0, 4.0, 1.0, 2.0, 3.0, 4.0, 0.0, 1.0, 4.0, 1.0, 1.0, 4.0, 1.0, 1.0, 0.0, 1.0, 1.0, 4.0, 1.0, 0.0, 1.0, 4.0, 1.0, 3.0, 3.0, 4.0, 1.0, 1.0, 3.0, 3.0, 3.0, 5.0, 3.0, 1.0, 1.0, 1.0, 4.0, 4.0, 4.0, 1.0, 5.0, 4.0, 2.0, 4.0, 4.0, 2.0, 1.0, 1.0, 0.0, 4.0, 1.0, 7.0, 1.0, 3.0, 4.0, 4.0, 1.0, 4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "predl=pred.tolist()\n",
    "print(predl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cebdb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 5, 3, 4, 1, 5, 0, 3, 1, 4, 5, 5, 4, 1, 4, 1, 1, 6, 1, 1, 0, 5, 2, 4, 2, 1, 1, 3, 1, 4, 4, 1, 4, 1, 1, 1, 5, 7, 4, 1, 1, 4, 2, 1, 1, 3, 3, 1, 1, 4, 1, 1, 1, 1, 5, 0, 4, 4, 3, 4, 7, 1, 1, 4, 1, 1, 4, 7, 0, 0, 1, 2, 3, 2, 3, 3, 2, 0, 0, 4, 4, 2, 0, 1, 3, 5, 2, 1, 0, 4, 3, 1, 4, 4, 0, 1, 4, 3, 4, 3, 1, 0, 4, 5, 4, 4, 3, 1, 4, 0, 4, 0, 2, 4, 6, 2, 1, 3, 4, 4, 3, 4, 1, 1, 5, 0, 0, 3, 2, 4, 5, 1, 4, 2, 1, 1, 4, 1, 1, 1, 5, 4, 3, 1, 1, 3, 4, 4, 1, 2, 0, 3, 1, 1, 4, 1, 2, 4, 1, 1, 5, 2, 1, 1, 1, 4, 4, 3, 6, 2, 0, 1, 2, 4, 1, 2, 1, 6, 1, 1, 4, 0, 1, 1, 6, 4, 4, 5, 1, 4, 4, 4, 4, 5, 3, 1, 4, 1, 2, 1, 4, 0, 1, 1, 5, 1, 5, 3, 0, 3, 4, 4, 4, 1, 7, 4, 4, 5, 1, 1, 2, 1, 3, 1, 5, 5, 1, 1, 1, 4, 4, 4, 1, 3, 1, 1, 2, 2, 1, 4, 1, 1, 4, 1, 2, 4, 5, 1, 2, 1, 3, 5, 1, 7, 1, 1, 4, 4, 2, 4, 3, 4, 1, 0, 3, 1, 4, 1, 5, 4, 0, 0, 1, 6, 5, 4, 1, 4, 4, 5, 3, 0, 2, 1, 4, 1, 3, 4, 3, 1, 1, 5, 5, 1, 0, 4, 4, 4, 3, 5, 1, 1, 3, 4, 5, 3, 2, 5, 3, 1, 5, 4, 1, 2, 5, 5, 4, 1, 1, 4, 4, 1, 1, 0, 4, 0, 4, 4, 2, 1, 2, 1, 4, 1, 2, 2, 2, 1, 3, 1, 0, 1, 4, 4, 4, 1, 3, 2, 7, 1, 4, 1, 4, 2, 5, 3, 0, 6, 1, 1, 1, 4, 0, 6, 2, 5, 4, 3, 1, 0, 4, 4, 4, 1, 3, 4, 3, 3, 1, 1, 4, 1, 1, 3, 1, 4, 0, 4, 0, 2, 4, 2, 4, 3, 0, 4, 2, 3, 1, 4, 1, 5, 3, 1, 2, 1, 4, 0, 1, 2, 1, 4, 1, 1, 4, 3, 1, 1, 0, 1, 3, 4, 1, 2, 2, 1, 4, 4, 6, 4, 4, 1, 1, 1, 4, 1, 3, 4, 1, 1, 4, 5, 5, 0, 4, 1, 4, 2, 1, 2, 4, 0, 3, 0, 4, 1, 3, 3, 3, 3, 0, 3, 1, 1, 0, 4, 4, 4, 1, 3, 4, 2, 4, 4, 6, 1, 3, 1, 4, 1, 7, 1, 1, 4, 4, 0, 4, 1, 1, 0, 1, 4, 3, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "test_Yl=test_Y.values.tolist()\n",
    "print(test_Yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83ee000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40573098273583086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(test_Yl,predl,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25fa0318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.90447\ttest-mlogloss:1.50108\n",
      "[1]\ttrain-mlogloss:0.59279\ttest-mlogloss:1.43615\n",
      "[2]\ttrain-mlogloss:0.38434\ttest-mlogloss:1.43987\n",
      "[3]\ttrain-mlogloss:0.25504\ttest-mlogloss:1.47199\n",
      "[4]\ttrain-mlogloss:0.18108\ttest-mlogloss:1.47226\n",
      "[5]\ttrain-mlogloss:0.13260\ttest-mlogloss:1.45206\n",
      "[6]\ttrain-mlogloss:0.10082\ttest-mlogloss:1.45418\n",
      "[7]\ttrain-mlogloss:0.07537\ttest-mlogloss:1.47660\n",
      "[8]\ttrain-mlogloss:0.05844\ttest-mlogloss:1.48850\n",
      "[9]\ttrain-mlogloss:0.04886\ttest-mlogloss:1.51417\n",
      "[10]\ttrain-mlogloss:0.04116\ttest-mlogloss:1.52514\n",
      "[11]\ttrain-mlogloss:0.03521\ttest-mlogloss:1.52783\n",
      "[12]\ttrain-mlogloss:0.02976\ttest-mlogloss:1.52415\n",
      "[13]\ttrain-mlogloss:0.02611\ttest-mlogloss:1.52661\n",
      "[14]\ttrain-mlogloss:0.02306\ttest-mlogloss:1.53481\n",
      "[15]\ttrain-mlogloss:0.02089\ttest-mlogloss:1.54638\n",
      "[16]\ttrain-mlogloss:0.01926\ttest-mlogloss:1.55703\n",
      "[17]\ttrain-mlogloss:0.01731\ttest-mlogloss:1.56295\n",
      "[18]\ttrain-mlogloss:0.01600\ttest-mlogloss:1.56856\n",
      "[19]\ttrain-mlogloss:0.01474\ttest-mlogloss:1.57009\n",
      "[20]\ttrain-mlogloss:0.01381\ttest-mlogloss:1.58073\n",
      "[21]\ttrain-mlogloss:0.01295\ttest-mlogloss:1.58990\n",
      "[22]\ttrain-mlogloss:0.01217\ttest-mlogloss:1.59751\n",
      "[23]\ttrain-mlogloss:0.01153\ttest-mlogloss:1.60216\n",
      "[24]\ttrain-mlogloss:0.01092\ttest-mlogloss:1.60464\n",
      "[25]\ttrain-mlogloss:0.01039\ttest-mlogloss:1.61268\n",
      "[26]\ttrain-mlogloss:0.00991\ttest-mlogloss:1.61837\n",
      "[27]\ttrain-mlogloss:0.00955\ttest-mlogloss:1.62767\n",
      "[28]\ttrain-mlogloss:0.00910\ttest-mlogloss:1.63171\n",
      "[29]\ttrain-mlogloss:0.00875\ttest-mlogloss:1.63651\n",
      "[30]\ttrain-mlogloss:0.00847\ttest-mlogloss:1.63747\n",
      "[31]\ttrain-mlogloss:0.00818\ttest-mlogloss:1.63946\n",
      "[32]\ttrain-mlogloss:0.00791\ttest-mlogloss:1.64262\n",
      "[33]\ttrain-mlogloss:0.00768\ttest-mlogloss:1.64201\n",
      "[34]\ttrain-mlogloss:0.00746\ttest-mlogloss:1.64881\n",
      "[35]\ttrain-mlogloss:0.00726\ttest-mlogloss:1.64864\n",
      "[36]\ttrain-mlogloss:0.00707\ttest-mlogloss:1.65330\n",
      "[37]\ttrain-mlogloss:0.00688\ttest-mlogloss:1.65075\n",
      "[38]\ttrain-mlogloss:0.00671\ttest-mlogloss:1.65429\n",
      "[39]\ttrain-mlogloss:0.00653\ttest-mlogloss:1.65527\n",
      "Test error using softprob = 0.3674698795180723\n"
     ]
    }
   ],
   "source": [
    "param['objective'] = 'multi:softprob'\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bst.predict(xg_test).reshape(len(test_Y), 8)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != test_Y) / len(test_Y)\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb03a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_l=pred_label.tolist()\n",
    "pred_label_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17bccc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40573098273583086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(recall_score(test_Yl,pred_label_l,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58feba8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2390c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f9292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
